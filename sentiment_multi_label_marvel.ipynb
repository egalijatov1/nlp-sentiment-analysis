{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-09T11:21:47.01047Z","iopub.status.busy":"2021-06-09T11:21:47.010081Z","iopub.status.idle":"2021-06-09T11:21:47.02146Z","shell.execute_reply":"2021-06-09T11:21:47.020505Z","shell.execute_reply.started":"2021-06-09T11:21:47.010435Z"},"trusted":true},"outputs":[],"source":["# text \n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import *\n","\n","# features\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n","\n","from collections import Counter\n","from wordcloud import WordCloud\n","\n","# Word2Vec\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","# metrics\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.svm import LinearSVC\n","from skmultilearn.adapt import MLkNN\n","from scipy.sparse import csr_matrix, lil_matrix\n","\n","# other\n","import numpy as np \n","import re\n","import nltk.data\n","import warnings\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import sklearn\n","from scipy import stats     \n","import csv"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:47.071521Z","iopub.status.busy":"2021-06-09T11:21:47.070961Z","iopub.status.idle":"2021-06-09T11:21:47.239605Z","shell.execute_reply":"2021-06-09T11:21:47.238562Z","shell.execute_reply.started":"2021-06-09T11:21:47.071475Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/data/mcu.csv')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:47.241695Z","iopub.status.busy":"2021-06-09T11:21:47.241407Z","iopub.status.idle":"2021-06-09T11:21:47.255198Z","shell.execute_reply":"2021-06-09T11:21:47.254173Z","shell.execute_reply.started":"2021-06-09T11:21:47.241666Z"},"trusted":true},"outputs":[],"source":["data['character'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["From above we see that there are 652 different characters. Since not all of them are known and important we can just do this analysis for 30 most important characters (most important being with largest number of dialogues)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:47.257042Z","iopub.status.busy":"2021-06-09T11:21:47.256651Z","iopub.status.idle":"2021-06-09T11:21:47.276615Z","shell.execute_reply":"2021-06-09T11:21:47.27535Z","shell.execute_reply.started":"2021-06-09T11:21:47.257009Z"},"trusted":true},"outputs":[],"source":["important_characters = np.array(data['character'].value_counts()[:15].keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:47.278849Z","iopub.status.busy":"2021-06-09T11:21:47.278345Z","iopub.status.idle":"2021-06-09T11:21:47.314429Z","shell.execute_reply":"2021-06-09T11:21:47.313299Z","shell.execute_reply.started":"2021-06-09T11:21:47.278802Z"},"trusted":true},"outputs":[],"source":["#removing rows with non-important characters\n","df = data.drop(data[~data.character.isin(important_characters)].index)\n","\n","#removing columns that we don't need\n","df = df.drop(columns=df.columns[6:])\n","df = df.drop(columns=['year'])\n","df = df.drop(columns=df.columns[0])\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:47.31737Z","iopub.status.busy":"2021-06-09T11:21:47.317046Z","iopub.status.idle":"2021-06-09T11:21:47.710267Z","shell.execute_reply":"2021-06-09T11:21:47.709211Z","shell.execute_reply.started":"2021-06-09T11:21:47.317341Z"},"trusted":true},"outputs":[],"source":["#character improtance\n","sns.set_style('whitegrid')\n","plt.figure(figsize=(10,10))\n","sns.countplot(y='character', data=df, order=df.character.value_counts().iloc[:15].index, palette=\"flare\")\n","plt.xlabel('Number of lines of dialogue', fontsize=15)\n","plt.ylabel('Character', fontsize=15)\n","plt.title('Character Importance by Number of Lines of Dialogue', fontsize=20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Of course this character importance depends deeply on the movies included in this dataset!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:47.712989Z","iopub.status.busy":"2021-06-09T11:21:47.712371Z","iopub.status.idle":"2021-06-09T11:21:48.03954Z","shell.execute_reply":"2021-06-09T11:21:48.03831Z","shell.execute_reply.started":"2021-06-09T11:21:47.71294Z"},"trusted":true},"outputs":[],"source":["total_char_words = df.groupby('character', as_index=False).words.sum()\n","total_char_words = pd.DataFrame(total_char_words)\n","sns.set_style('whitegrid')\n","plt.figure(figsize=(10,10))\n","sns.barplot(x='words',y='character', data=total_char_words, palette=\"flare\", order=total_char_words.sort_values('words', ascending=False).character[:15], orient='h')\n","plt.xlabel('Number of words of dialogue', fontsize=15)\n","plt.ylabel('Character', fontsize=15)\n","plt.title('Character Importance by Number of Words in Dialogues', fontsize=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.042023Z","iopub.status.busy":"2021-06-09T11:21:48.041571Z","iopub.status.idle":"2021-06-09T11:21:48.362366Z","shell.execute_reply":"2021-06-09T11:21:48.36104Z","shell.execute_reply.started":"2021-06-09T11:21:48.04198Z"},"trusted":true},"outputs":[],"source":["sns.set_style('whitegrid')\n","plt.figure(figsize=(10,10))\n","sns.countplot(y='movie', data=df, order=df.movie.value_counts().iloc[:11].index, palette=\"flare\")\n","plt.xlabel('Number of lines of dialogue', fontsize=15)\n","plt.ylabel('Movie', fontsize=15)\n","plt.title('Distribution of dialogue lines in movies', fontsize=20)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.365348Z","iopub.status.busy":"2021-06-09T11:21:48.364593Z","iopub.status.idle":"2021-06-09T11:21:48.377745Z","shell.execute_reply":"2021-06-09T11:21:48.37646Z","shell.execute_reply.started":"2021-06-09T11:21:48.365296Z"},"trusted":true},"outputs":[],"source":["df['words'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.379862Z","iopub.status.busy":"2021-06-09T11:21:48.379414Z","iopub.status.idle":"2021-06-09T11:21:48.397672Z","shell.execute_reply":"2021-06-09T11:21:48.396214Z","shell.execute_reply.started":"2021-06-09T11:21:48.379819Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing training dataset "]},{"cell_type":"markdown","metadata":{},"source":["Creating csv with multi-target columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.399477Z","iopub.status.busy":"2021-06-09T11:21:48.399143Z","iopub.status.idle":"2021-06-09T11:21:48.410702Z","shell.execute_reply":"2021-06-09T11:21:48.40965Z","shell.execute_reply.started":"2021-06-09T11:21:48.399447Z"},"trusted":true},"outputs":[],"source":["tsv_file = open(\"/data/en-annotated.tsv\")\n","read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n","\n","column_names = ['text', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n","df = pd.DataFrame(columns = column_names)\n","i = 0\n","for row in read_tsv:\n","    arr = pd.DataFrame([[row[0],0,0,0,0,0,0,0,0]],columns = list(column_names), index = [i])\n","    nums = row[1].split(\",\")\n","    for num in nums:\n","        arr.loc[i, column_names[int(num)]]  = 1\n","    df = pd.concat([df,arr])\n","    i+=1\n","\n","# save to csv so that you don't have to run it every time\n","df.to_csv('/data/sentiment.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.414037Z","iopub.status.busy":"2021-06-09T11:21:48.413349Z","iopub.status.idle":"2021-06-09T11:21:48.456625Z","shell.execute_reply":"2021-06-09T11:21:48.455389Z","shell.execute_reply.started":"2021-06-09T11:21:48.413881Z"},"trusted":true},"outputs":[],"source":["# read from csv\n","mov_data = pd.read_csv('/data/sentiment.csv')\n","categories = list(mov_data.columns[1:].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.45879Z","iopub.status.busy":"2021-06-09T11:21:48.458306Z","iopub.status.idle":"2021-06-09T11:21:48.760564Z","shell.execute_reply":"2021-06-09T11:21:48.759706Z","shell.execute_reply.started":"2021-06-09T11:21:48.458722Z"},"trusted":true},"outputs":[],"source":["sns.set(font_scale = 2)\n","plt.figure(figsize=(15,11))\n","ax = sns.barplot(categories, mov_data.iloc[:,1:].sum().values)\n","\n","plt.title(\"Lines in each category\", fontsize=24)\n","plt.ylabel('Number of lines', fontsize=18)\n","plt.xlabel('Line Type ', fontsize=18)\n","#adding the text labels\n","rects = ax.patches\n","labels = mov_data.iloc[:,1:].sum().values\n","for rect, label in zip(rects, labels):\n","    height = rect.get_height()\n","    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.762627Z","iopub.status.busy":"2021-06-09T11:21:48.762283Z","iopub.status.idle":"2021-06-09T11:21:48.790143Z","shell.execute_reply":"2021-06-09T11:21:48.78883Z","shell.execute_reply.started":"2021-06-09T11:21:48.762595Z"},"trusted":true},"outputs":[],"source":["def num_word(line):\n","    return len(line.split())\n","\n","mov_data['words'] = mov_data['text'].apply(num_word)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:48.791948Z","iopub.status.busy":"2021-06-09T11:21:48.79159Z","iopub.status.idle":"2021-06-09T11:21:48.807426Z","shell.execute_reply":"2021-06-09T11:21:48.806091Z","shell.execute_reply.started":"2021-06-09T11:21:48.791916Z"},"trusted":true},"outputs":[],"source":["mov_data['words'].describe()"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:49.995013Z","iopub.status.busy":"2021-06-09T11:21:49.994387Z","iopub.status.idle":"2021-06-09T11:21:50.002216Z","shell.execute_reply":"2021-06-09T11:21:50.001045Z","shell.execute_reply.started":"2021-06-09T11:21:49.994961Z"},"trusted":true},"outputs":[],"source":["import spacy\n","\n","def lemmatize(text):\n","    nlp = spacy.load('en', disable=['parser', 'ner'])\n","    doc = nlp(text)\n","    return (\" \".join([token.lemma_ for token in doc]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.005113Z","iopub.status.busy":"2021-06-09T11:21:50.004368Z","iopub.status.idle":"2021-06-09T11:21:50.033325Z","shell.execute_reply":"2021-06-09T11:21:50.032175Z","shell.execute_reply.started":"2021-06-09T11:21:50.005052Z"},"trusted":true},"outputs":[],"source":["def preprocess(text):\n","    \n","    # lemmatize sentences \n","    lemmatized = lemmatize(text)\n","    lemmatized = re.sub(\" -PRON-\", \"\", lemmatized)\n","    \n","    # leave only letters\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", lemmatized)\n","       \n","    # remove stop words\n","    words = letters_only.lower().split()           \n","    stops = set(stopwords.words(\"english\")) \n","    meaningful_words = [w for w in words if not w in stops] \n","\n","    return(\" \".join( meaningful_words ))"]},{"cell_type":"markdown","metadata":{},"source":["Preprocessing MARVEL dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.035364Z","iopub.status.busy":"2021-06-09T11:21:50.035007Z","iopub.status.idle":"2021-06-09T11:21:50.055333Z","shell.execute_reply":"2021-06-09T11:21:50.054097Z","shell.execute_reply.started":"2021-06-09T11:21:50.035329Z"},"trusted":true},"outputs":[],"source":["df['clean_line'] = df['line'].apply(preprocess)  \n","df = df.drop(a[a.clean_line.isin([\"\"])].index) \n","df.to_csv('/data/marvel_dataset.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["Preprocessing training dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.057729Z","iopub.status.busy":"2021-06-09T11:21:50.057056Z","iopub.status.idle":"2021-06-09T11:21:50.089382Z","shell.execute_reply":"2021-06-09T11:21:50.088423Z","shell.execute_reply.started":"2021-06-09T11:21:50.057687Z"},"trusted":true},"outputs":[],"source":["## Preprocessed data is saved to file so that preprocessing does not have to be repeated!\n","\n","mov_data['clean_text'] = mov_data['text'].apply(preprocess)\n","mov_data = mov_data.drop(mov_data[mov_data.clean_text.isin([\"\"])].index)\n","mov_data.to_csv('/data/preprocessed.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# FEATURES EXTRACTION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.091334Z","iopub.status.busy":"2021-06-09T11:21:50.09079Z","iopub.status.idle":"2021-06-09T11:21:50.111737Z","shell.execute_reply":"2021-06-09T11:21:50.110838Z","shell.execute_reply.started":"2021-06-09T11:21:50.091299Z"},"trusted":true},"outputs":[],"source":["def word_grams(cleans): \n","    word_vectorizer = TfidfVectorizer(analyzer = \"word\", ngram_range=(1,3), tokenizer = None, preprocessor = None,\n","                                      stop_words = None, max_features = 300, max_df = 0.90) \n","    wgram_features = word_vectorizer.fit_transform(cleans)\n","    return wgram_features.toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.113802Z","iopub.status.busy":"2021-06-09T11:21:50.113286Z","iopub.status.idle":"2021-06-09T11:21:50.132998Z","shell.execute_reply":"2021-06-09T11:21:50.131819Z","shell.execute_reply.started":"2021-06-09T11:21:50.113737Z"},"trusted":true},"outputs":[],"source":["def char_grams(cleans):\n","    char_vectorizer = TfidfVectorizer(analyzer = \"char\", ngram_range=(1,3), tokenizer = None, preprocessor = None,\n","                                      stop_words = None, max_features = 200, max_df = 0.85)\n","    cgram_features = char_vectorizer.fit_transform(cleans)\n","    return cgram_features.toarray()"]},{"cell_type":"markdown","metadata":{},"source":["Number of syllables in a word (stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.134925Z","iopub.status.busy":"2021-06-09T11:21:50.134407Z","iopub.status.idle":"2021-06-09T11:21:50.154231Z","shell.execute_reply":"2021-06-09T11:21:50.153279Z","shell.execute_reply.started":"2021-06-09T11:21:50.13489Z"},"trusted":true},"outputs":[],"source":["def syllable_count(word):\n","    word = word.lower()\n","    count = 0\n","    vowels = \"aeiouy\"\n","    if word[0] in vowels:\n","        count += 1\n","    for index in range(1, len(word)):\n","        if word[index] in vowels and word[index - 1] not in vowels:\n","            count += 1\n","    if word.endswith(\"e\"):\n","        count -= 1\n","    if count == 0:\n","        count += 1\n","    return count"]},{"cell_type":"markdown","metadata":{},"source":["Function for extraction of linguistic features:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.15628Z","iopub.status.busy":"2021-06-09T11:21:50.155676Z","iopub.status.idle":"2021-06-09T11:21:50.175593Z","shell.execute_reply":"2021-06-09T11:21:50.174721Z","shell.execute_reply.started":"2021-06-09T11:21:50.156224Z"},"trusted":true},"outputs":[],"source":["def ling_features(line, clean):\n","   \n","    # get raw text\n","    words = clean.split() \n","    \n","    syllables_all = syllable_count(clean)\n","    num_chars = sum(len(w) for w in words)\n","    num_chars_total = len(line) \n","    num_terms = len(line.split()) \n","    num_words = len(words) \n","    num_unique_terms = len(set(words))    \n","    \n","    # FKRA and FRE\n","    avg_syl = round(float((syllables_all+0.001))/float(num_words+0.001),4)\n","    \n","    # Modified FK grade, where avg words per sentence is just num words/1\n","    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n","    \n","    # Modified FRE score, where sentence fixed to 1\n","    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n","        \n","    return [FKRA, FRE,syllables_all, avg_syl, num_chars, num_chars_total, num_terms, num_words,num_unique_terms]\n","\n","\n","def get_ling_feature_array(lines, cleans):\n","    feats=[] \n","    for (line, clean) in zip(lines, cleans):\n","        feats.append(ling_features(line, clean))\n","    return np.array(feats)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.177524Z","iopub.status.busy":"2021-06-09T11:21:50.177227Z","iopub.status.idle":"2021-06-09T11:21:50.201221Z","shell.execute_reply":"2021-06-09T11:21:50.200331Z","shell.execute_reply.started":"2021-06-09T11:21:50.177496Z"},"trusted":true},"outputs":[],"source":["from nltk.corpus import opinion_lexicon\n","\n","def sentiment_features(text):\n","    pos_words = 0\n","    neg_words = 0\n","    pos_start = 1000\n","    neg_start = 1000\n","    pos_set = set(opinion_lexicon.positive())\n","    neg_set = set(opinion_lexicon.negative())\n","    i = 0\n","    for word in text.split():\n","        if word in pos_set:\n","            pos_words += 1\n","            if pos_start >= 1000:\n","                pos_start = i\n","            \n","        elif word in neg_set:\n","            neg_words += 1\n","            if neg_start >= 1000:\n","                neg_start = i\n","        i += 1\n","    \n","    if neg_words != 0:\n","        ratio = pos_words/neg_words\n","    else:\n","        ratio = pos_words\n","        \n","    return [pos_words, neg_words, ratio, pos_start, neg_start]\n","\n","\n","def get_sentiment_feature_array(lines):\n","    feats=[]\n","    for line in lines:\n","        feats.append(sentiment_features(line))\n","    return np.array(feats)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.203079Z","iopub.status.busy":"2021-06-09T11:21:50.20255Z","iopub.status.idle":"2021-06-09T11:21:50.228952Z","shell.execute_reply":"2021-06-09T11:21:50.227528Z","shell.execute_reply.started":"2021-06-09T11:21:50.203028Z"},"trusted":true},"outputs":[],"source":["def pos_features(cleans):\n","    pos_vectorizer = TfidfVectorizer(tokenizer=None, lowercase=False,preprocessor=None,ngram_range=(1, 3), stop_words=None,\n","                                     use_idf=False, smooth_idf=False, norm=None, decode_error='replace', max_features=50,\n","                                     min_df=0.1,max_df=0.80)\n","    text_tags = []\n","    for c in cleans:\n","        tokens = c\n","        tags = nltk.pos_tag(tokens)\n","        tag_list = [x[1] for x in tags]\n","        tag_str = \" \".join(tag_list) \n","        text_tags.append(tag_str) \n","        \n","    return pos_vectorizer.fit_transform(pd.Series(text_tags)).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.231236Z","iopub.status.busy":"2021-06-09T11:21:50.230658Z","iopub.status.idle":"2021-06-09T11:21:50.247192Z","shell.execute_reply":"2021-06-09T11:21:50.246209Z","shell.execute_reply.started":"2021-06-09T11:21:50.231197Z"},"trusted":true},"outputs":[],"source":["def read_data(file_name):\n","    with open(file_name,'r') as f:\n","        word_vocab = set() # not using list to avoid duplicate entry\n","        word2vector = {}\n","        for line in f:\n","            line_ = line.strip() #Remove white space\n","            words_Vec = line_.split()\n","            word_vocab.add(words_Vec[0])\n","            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n","    print(\"Total Words in DataSet:\",len(word_vocab))\n","    return word_vocab,word2vector\n","\n","def average_word_vectors(words, model, vocabulary, num_features):\n","    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","    nwords = 0.\n","    for word in words:\n","        if word in vocabulary: \n","            nwords = nwords + 1.\n","            feature_vector = np.add(feature_vector, model[word])\n","    if nwords:\n","        feature_vector = np.divide(feature_vector, nwords)\n","    return feature_vector  \n","   \n","def averaged_word_vectorizer(corpus, model, vocab, num_features):\n","    vocabulary = set(vocab)\n","    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n","    return np.array(features)"]},{"cell_type":"markdown","metadata":{},"source":["Features extraction of training dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.248851Z","iopub.status.busy":"2021-06-09T11:21:50.24849Z","iopub.status.idle":"2021-06-09T11:21:50.327359Z","shell.execute_reply":"2021-06-09T11:21:50.326149Z","shell.execute_reply.started":"2021-06-09T11:21:50.248816Z"},"trusted":true},"outputs":[],"source":["# load preprocessed data of training dataset\n","\n","prep_data = pd.read_csv('/data/preprocessed.csv')\n","prep_data = prep_data.drop(prep_data[prep_data.clean_text.isna()].index)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.329646Z","iopub.status.busy":"2021-06-09T11:21:50.329182Z","iopub.status.idle":"2021-06-09T11:21:50.335272Z","shell.execute_reply":"2021-06-09T11:21:50.33394Z","shell.execute_reply.started":"2021-06-09T11:21:50.329599Z"},"trusted":true},"outputs":[],"source":["wordgram_features = word_grams(prep_data.clean_text)\n","char_gram_features = char_grams(prep_data.clean_text)\n","ling_feats = get_ling_feature_array(prep_data.text, prep_data.clean_text) \n","sentiment_feats = get_sentiment_feature_array(prep_data.text) \n","pos_feats = pos_features(prep_data.clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:21:50.33722Z","iopub.status.busy":"2021-06-09T11:21:50.336877Z","iopub.status.idle":"2021-06-09T11:22:07.626434Z","shell.execute_reply":"2021-06-09T11:22:07.625553Z","shell.execute_reply.started":"2021-06-09T11:21:50.337188Z"},"trusted":true},"outputs":[],"source":["vocab, w2v = read_data(\"/data/glove.6B.100d.txt\")\n","embeddings = averaged_word_vectorizer(corpus=prep_data.clean_text, model=w2v, vocab=vocab, num_features=100)\n","emb_df = pd.DataFrame(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:22:07.628358Z","iopub.status.busy":"2021-06-09T11:22:07.628059Z","iopub.status.idle":"2021-06-09T11:22:07.632263Z","shell.execute_reply":"2021-06-09T11:22:07.631307Z","shell.execute_reply.started":"2021-06-09T11:22:07.628332Z"},"trusted":true},"outputs":[],"source":["all_features = np.concatenate([wordgram_features, char_gram_features, ling_feats, sentiment_feats, pos_feats],axis=1)\n","X = pd.DataFrame(all_features)\n","X.to_csv(\"/data/features.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["Features extraction from MARVEL dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:22:07.633692Z","iopub.status.busy":"2021-06-09T11:22:07.63341Z","iopub.status.idle":"2021-06-09T11:22:07.670997Z","shell.execute_reply":"2021-06-09T11:22:07.669863Z","shell.execute_reply.started":"2021-06-09T11:22:07.633667Z"},"trusted":true},"outputs":[],"source":["marvel_prep = pd.read_csv('/data/marvel_dataset.csv')\n","\n","# vocab, w2v = read_data(\"/data/glove.6B.100d.txt\")\n","mar_embeddings = averaged_word_vectorizer(corpus=marvel_prep.clean_line, model=w2v, vocab=vocab, num_features=100)\n","mar_emb_df = pd.DataFrame(mar_embeddings)\n","\n","mar_wordgram_features = word_grams(marvel_prep.clean_line)\n","mar_char_gram_features = char_grams(marvel_prep.clean_line)\n","mar_ling_feats = get_ling_feature_array(marvel_prep.line, marvel_prep.clean_line) \n","mar_sentiment_feats = get_sentiment_feature_array(marvel_prep.line) \n","mar_pos_feats = pos_features(marvel_prep.clean_line)\n","\n","mar_all_features = np.concatenate([mar_wordgram_features, mar_char_gram_features, mar_ling_feats, mar_sentiment_feats, mar_pos_feats],axis=1)\n","mar_X = pd.DataFrame(mar_all_features)\n","mar_X = pd.concat([mar_X, mar_emb_df], axis=1)\n","mar_X.to_csv(\"/data/marvel_features.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# MODEL TRAINING"]},{"cell_type":"markdown","metadata":{},"source":["Loading features from file (training dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:22:07.672413Z","iopub.status.busy":"2021-06-09T11:22:07.672133Z","iopub.status.idle":"2021-06-09T11:22:09.18033Z","shell.execute_reply":"2021-06-09T11:22:09.179366Z","shell.execute_reply.started":"2021-06-09T11:22:07.672387Z"},"trusted":true},"outputs":[],"source":["X = pd.read_csv('/data/features.csv')\n","X = X.drop(labels = ['Unnamed: 0'], axis = 1)\n","X = pd.concat([X, emb_df], axis=1)\n","y = prep_data.drop(labels = ['text','clean_text'], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Loading features from file (MARVEL dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:22:09.182155Z","iopub.status.busy":"2021-06-09T11:22:09.181826Z","iopub.status.idle":"2021-06-09T11:22:09.959294Z","shell.execute_reply":"2021-06-09T11:22:09.958304Z","shell.execute_reply.started":"2021-06-09T11:22:09.182119Z"},"trusted":true},"outputs":[],"source":["mar_X = pd.read_csv('/data/marvel_features.csv')\n","mar_X = mar_X.drop(labels = ['Unnamed: 0'], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T11:52:40.661979Z","iopub.status.busy":"2021-06-09T11:52:40.661459Z","iopub.status.idle":"2021-06-09T11:52:40.741452Z","shell.execute_reply":"2021-06-09T11:52:40.740277Z","shell.execute_reply.started":"2021-06-09T11:52:40.661929Z"},"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.2)\n","columns = ['category', 'test_accuracy', 'train_accuracy']"]},{"cell_type":"markdown","metadata":{},"source":["OneVsRestClassifier with Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T12:09:19.237229Z","iopub.status.busy":"2021-06-09T12:09:19.236889Z","iopub.status.idle":"2021-06-09T12:10:32.31392Z","shell.execute_reply":"2021-06-09T12:10:32.313111Z","shell.execute_reply.started":"2021-06-09T12:09:19.237199Z"},"trusted":true},"outputs":[],"source":["LogReg_pipeline = Pipeline([\n","                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n","            ])\n","\n","\n","tps = 0\n","fps = 0\n","fns = 0\n","prc = 0\n","rec = 0\n","results_logreg = pd.DataFrame(columns = columns)\n","i = 0\n","\n","character_sentiment1 = marvel_prep.drop(labels = ['clean_line', 'words'], axis=1)\n","for category in categories:\n","    print('**Processing ' + category + '**')\n","    \n","    LogReg_pipeline.fit(X_train, y_train[category])\n","    \n","    test_prediction = LogReg_pipeline.predict(X_test)\n","    train_prediction = LogReg_pipeline.predict(X_train)\n","    marv_predict = LogReg_pipeline.predict(mar_X)\n","    marv_pred_df = pd.DataFrame(marv_predict, columns=[category])\n","    \n","    test_accuracy = accuracy_score(y_test[category], test_prediction)\n","    train_accuracy = accuracy_score(y_train[category], train_prediction)\n","    cf = confusion_matrix(y_test[category], test_prediction)\n","    tp = cf[0][0]\n","    fp = cf[0][1]\n","    fn = cf[1][0]\n","    tps += tp\n","    fps += fp\n","    fns += fn\n","    prca = tp/(tp+fp)\n","    reca = tp/(tp+fn)\n","    prc += prca\n","    rec += reca\n","    \n","    character_sentiment1 = pd.concat([character_sentiment1, marv_pred_df], axis=1)\n","    \n","    #save results\n","    temp = pd.DataFrame([[category, test_accuracy, train_accuracy]],columns = list(columns), index = [i])\n","    results_logreg = pd.concat([results_logreg, temp])\n","    i+=1\n","\n","print(\"Micro averaging precision: \", tps/(tps+fps))\n","print(\"Micro averaging recall: \", tps/(tps+fns))\n","print(\"Macro averaging precision: \", prc/len(categories))\n","print(\"Macro averaging recall: \", rec/len(categories))\n","\n","results_logreg"]},{"cell_type":"markdown","metadata":{},"source":["OneVsRestClassifier with SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T12:12:42.930632Z","iopub.status.busy":"2021-06-09T12:12:42.930279Z","iopub.status.idle":"2021-06-09T12:14:25.151889Z","shell.execute_reply":"2021-06-09T12:14:25.150859Z","shell.execute_reply.started":"2021-06-09T12:12:42.930603Z"},"trusted":true},"outputs":[],"source":["SVC_pipeline = Pipeline([\n","                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=-1)),\n","            ])\n","\n","tps = 0\n","fps = 0\n","fns = 0\n","prc = 0\n","rec = 0\n","results_svc = pd.DataFrame(columns = columns)\n","i = 0\n","\n","character_sentiment2 = marvel_prep.drop(labels = ['clean_line','words'], axis=1)\n","\n","for category in categories:\n","    print('**Processing ' + category + '**')\n","    \n","    SVC_pipeline.fit(X_train, y_train[category])\n","    \n","    test_prediction = SVC_pipeline.predict(X_test)\n","    train_prediction = SVC_pipeline.predict(X_train)\n","    marv_predict = SVC_pipeline.predict(mar_X)\n","    marv_pred_df = pd.DataFrame(marv_predict, columns=[category])\n","    \n","    test_accuracy = accuracy_score(y_test[category], test_prediction)\n","    train_accuracy = accuracy_score(y_train[category], train_prediction)\n","    cf = confusion_matrix(y_test[category], test_prediction)\n","    tp = cf[0][0]\n","    fp = cf[0][1]\n","    fn = cf[1][0]\n","    tps += tp\n","    fps += fp\n","    fns += fn\n","    prca = tp/(tp+fp)\n","    reca = tp/(tp+fn)\n","    prc += prca\n","    rec += reca\n","    character_sentiment2 = pd.concat([character_sentiment2, marv_pred_df], axis=1)\n","    \n","    #save results\n","    temp = pd.DataFrame([[category, test_accuracy, train_accuracy]],columns = list(columns), index = [i])\n","    results_svc = pd.concat([results_svc, temp])\n","    i+=1\n","    \n","# results for SVM\n","print(\"Micro averaging precision: \", tps/(tps+fps))\n","print(\"Micro averaging recall: \", tps/(tps+fns))\n","print(\"Macro averaging precision: \", prc/len(categories))\n","print(\"Macro averaging recall: \", rec/len(categories))\n","results_svc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:25:03.509381Z","iopub.status.busy":"2021-06-09T13:25:03.508831Z","iopub.status.idle":"2021-06-09T13:25:03.527204Z","shell.execute_reply":"2021-06-09T13:25:03.526124Z","shell.execute_reply.started":"2021-06-09T13:25:03.509336Z"},"trusted":true},"outputs":[],"source":["df_tony1 = character_sentiment1.drop(character_sentiment1[~character_sentiment1.character.isin(['TONY STARK'])].index)\n","df_tony1 = df_tony1.drop(labels = ['character','line','movie'], axis=1)\n","df_tony1.sum(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:25:08.031304Z","iopub.status.busy":"2021-06-09T13:25:08.03095Z","iopub.status.idle":"2021-06-09T13:25:08.499207Z","shell.execute_reply":"2021-06-09T13:25:08.498049Z","shell.execute_reply.started":"2021-06-09T13:25:08.031275Z"},"trusted":true},"outputs":[],"source":["df_tony1.sum(axis=0).plot.bar()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:46:09.610085Z","iopub.status.busy":"2021-06-09T13:46:09.609554Z","iopub.status.idle":"2021-06-09T13:46:09.627675Z","shell.execute_reply":"2021-06-09T13:46:09.626926Z","shell.execute_reply.started":"2021-06-09T13:46:09.610051Z"},"trusted":true},"outputs":[],"source":["df_steve1 = character_sentiment1.drop(character_sentiment1[~character_sentiment1.character.isin(['STEVE ROGERS'])].index)\n","df_steve1 = df_steve1.drop(labels = ['character','line','movie'], axis=1)\n","df_steve1.sum(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:46:12.661058Z","iopub.status.busy":"2021-06-09T13:46:12.660514Z","iopub.status.idle":"2021-06-09T13:46:12.849697Z","shell.execute_reply":"2021-06-09T13:46:12.848904Z","shell.execute_reply.started":"2021-06-09T13:46:12.661025Z"},"trusted":true},"outputs":[],"source":["df_steve1.sum(axis=0).plot.bar()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:22:05.598802Z","iopub.status.busy":"2021-06-09T13:22:05.598417Z","iopub.status.idle":"2021-06-09T13:22:05.615449Z","shell.execute_reply":"2021-06-09T13:22:05.614287Z","shell.execute_reply.started":"2021-06-09T13:22:05.598754Z"},"trusted":true},"outputs":[],"source":["df_tony2 = character_sentiment2.drop(character_sentiment2[~character_sentiment2.character.isin(['TONY STARK'])].index)\n","df_tony2 = df_tony2.drop(labels = ['character','line','movie'], axis=1)\n","df_tony2.sum(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:45:59.92748Z","iopub.status.busy":"2021-06-09T13:45:59.926913Z","iopub.status.idle":"2021-06-09T13:45:59.945844Z","shell.execute_reply":"2021-06-09T13:45:59.944625Z","shell.execute_reply.started":"2021-06-09T13:45:59.927446Z"},"trusted":true},"outputs":[],"source":["df_tony2.sum(axis=0).plot.bar()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:44:31.515108Z","iopub.status.busy":"2021-06-09T13:44:31.514711Z","iopub.status.idle":"2021-06-09T13:44:31.53204Z","shell.execute_reply":"2021-06-09T13:44:31.530989Z","shell.execute_reply.started":"2021-06-09T13:44:31.515073Z"},"trusted":true},"outputs":[],"source":["df_steve2 = character_sentiment2.drop(character_sentiment2[~character_sentiment2.character.isin(['STEVE ROGERS'])].index)\n","df_steve2 = df_steve2.drop(labels = ['character','line','movie'], axis=1)\n","df_steve2.sum(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:44:35.290776Z","iopub.status.busy":"2021-06-09T13:44:35.290245Z","iopub.status.idle":"2021-06-09T13:44:35.475619Z","shell.execute_reply":"2021-06-09T13:44:35.474916Z","shell.execute_reply.started":"2021-06-09T13:44:35.290731Z"},"trusted":true},"outputs":[],"source":["df_steve2.sum(axis=0).plot.bar()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
